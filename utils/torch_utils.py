# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license
"""
    PyTorch utils
    è¿™ä¸ªæ–‡ä»¶ä¸»è¦æ˜¯åŸºäºŽtorchçš„ä¸€äº›å®žç”¨å·¥å…·ç±»ï¼Œæ•´ä¸ªé¡¹ç›®çš„æ–‡ä»¶éƒ½å¯èƒ½ä¼šç”¨åˆ°ï¼Œå¹¶ä¸æ¶‰åŠå¤ªå¤šçš„çŸ©é˜µæ“ä½œï¼Œå¤§å¤šéƒ½æ˜¯ä¸€äº›torchç›¸å…³å·¥å…·çš„ä½¿ç”¨ã€‚
"""

"""
    Â·æ³¨é‡Šæ¥æºäºŽå„ä½å¤§ä½¬çš„è§†é¢‘+åšå®¢ï¼Œæ”¶é›†ä¸æ˜“ï¼Œç¥ä½ æ—©æ—¥å‡ºsciï¼
    Â·ç§‰æŒå¼€æºç²¾ç¥žï¼å–ä¹‹äºŽå¤§ä½¬ï¼Œç”¨ä¹‹äºŽå„ä½ï¼
    Â·@Dragon AI 
"""

import datetime # æ—¶é—´æ¨¡å—  åŸºäºŽtimeè¿›è¡Œäº†å°è£… æ›´é«˜çº§
import math  # æ•°å­¦å‡½æ•°æ¨¡å—
import os    # ä¸Žæ“ä½œç³»ç»Ÿè¿›è¡Œäº¤äº’çš„æ¨¡å—
import platform # æä¾›èŽ·å–æ“ä½œç³»ç»Ÿç›¸å…³ä¿¡æ¯çš„æ¨¡å—
import subprocess   # å­è¿›ç¨‹å®šä¹‰åŠæ“ä½œçš„æ¨¡å—
import time  # æ—¶é—´æ¨¡å— æ›´åº•å±‚
import warnings
from contextlib import contextmanager    # ç”¨äºŽè¿›è¡Œä¸Šä¸‹æ–‡ç®¡ç†çš„æ¨¡å—
from copy import deepcopy   # å®žçŽ°æ·±åº¦å¤åˆ¶çš„æ¨¡å—
from pathlib import Path     # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºŽæ“ä½œçš„æ¨¡å—

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from utils.general import LOGGER

try:
    import thop  # ç”¨äºŽPytorchæ¨¡åž‹çš„FLOPSè®¡ç®—å·¥å…·æ¨¡å—
except ImportError:
    thop = None

# Suppress PyTorch warnings
warnings.filterwarnings('ignore', message='User provided device_type of \'cuda\', but CUDA is not available. Disabling')


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """
        å‡½æ•°åŠŸèƒ½ï¼šå¤„ç†æ¨¡åž‹è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒæ—¶çš„åŒæ­¥é—®é¢˜
        åŽŸç†ï¼šåŸºäºŽtorch.distributed.barrier()å‡½æ•°çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä¸ºäº†å®Œæˆæ•°æ®çš„æ­£å¸¸åŒæ­¥æ“ä½œï¼ˆyolov5ä¸­æ‹¥æœ‰å¤§é‡çš„å¤šçº¿ç¨‹å¹¶è¡Œæ“ä½œï¼‰
        è¢«è°ƒç”¨ï¼štrain.py
    """

    """
        :params local_rank: ä»£è¡¨å½“å‰è¿›ç¨‹å·  0ä»£è¡¨ä¸»è¿›ç¨‹  1ã€2ã€3ä»£è¡¨å­è¿›ç¨‹
    """
    if local_rank not in [-1, 0]:
        # å¦‚æžœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹ä¸æ˜¯ä¸»è¿›ç¨‹ï¼Œå³rankä¸ç­‰äºŽ0æˆ–è€…-1ï¼Œ
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šæ‰§è¡Œç›¸åº”çš„torch.distributed.barrier()ï¼Œè®¾ç½®ä¸€ä¸ªé˜»å¡žæ …æ ï¼Œ
        # è®©æ­¤è¿›ç¨‹å¤„äºŽç­‰å¾…çŠ¶æ€ï¼Œç­‰å¾…æ‰€æœ‰è¿›ç¨‹åˆ°è¾¾æ …æ å¤„ï¼ˆåŒ…æ‹¬ä¸»è¿›ç¨‹æ•°æ®å¤„ç†å®Œæ¯•ï¼‰ï¼›
        dist.barrier(device_ids=[local_rank])
    yield   # yieldè¯­å¥ ä¸­æ–­åŽæ‰§è¡Œä¸Šä¸‹æ–‡ä»£ç ï¼Œç„¶åŽè¿”å›žåˆ°æ­¤å¤„ç»§ç»­å¾€ä¸‹æ‰§è¡Œ
    if local_rank == 0:
        # å¦‚æžœæ‰§è¡Œcreate_dataloader()å‡½æ•°çš„è¿›ç¨‹æ˜¯ä¸»è¿›ç¨‹ï¼Œå…¶ä¼šç›´æŽ¥åŽ»è¯»å–æ•°æ®å¹¶å¤„ç†ï¼Œ
        # ç„¶åŽå…¶å¤„ç†ç»“æŸä¹‹åŽä¼šæŽ¥ç€é‡åˆ°torch.distributed.barrier()ï¼Œ
        # æ­¤æ—¶ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½åˆ°è¾¾äº†å½“å‰çš„æ …æ å¤„ï¼Œè¿™æ ·æ‰€æœ‰è¿›ç¨‹å°±è¾¾åˆ°äº†åŒæ­¥ï¼Œå¹¶åŒæ—¶å¾—åˆ°é‡Šæ”¾ã€‚
        dist.barrier(device_ids=[0])

# ==============================================ä¸‹é¢ä¸‰ä¸ªå‡½æ•°æ˜¯ä¸€èµ·å®Œæˆè‡ªåŠ¨é€‰æ‹©ç³»ç»Ÿè®¾å¤‡çš„æ“ä½œ=================================


def git_describe(path=Path(__file__).parent):  # path must be a directory
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨äºŽè¿”å›žpathæ–‡ä»¶å¯è¯»çš„gitæè¿°
        æºç åœ°å€ï¼šhttps://git-scm.com/docs/git-describe
        è¢«è°ƒç”¨ï¼šselect_deviceå‡½æ•°ä¸­ã€‚
    """
    """
        path: éœ€è¦åœ¨gitä¸­æŸ¥è¯¢ï¼ˆæ–‡ä»¶æè¿°ï¼‰çš„æ–‡ä»¶å  é»˜è®¤å½“å‰æ–‡ä»¶çš„çˆ¶è·¯å¾„
    """
    # return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe
    s = f'git -C {path} describe --tags --long --always'
    try:
        # åˆ›å»ºä¸€ä¸ªå­è¿›ç¨‹åœ¨å‘½ä»¤è¡Œæ‰§è¡Œ s(git) å‘½ä»¤(è¿”å›žpathæ–‡ä»¶çš„æè¿°) è¿”å›žæ‰§è¡Œç»“æžœ(pathæ–‡ä»¶çš„æè¿°)
        return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]
    except subprocess.CalledProcessError:
        # å‘ç”Ÿå¼‚å¸¸ path not a git repository è¿”å›ž''
        return ''  # not a git repository

def date_modified(path=__file__):
    """
        å‡½æ•°åŠŸèƒ½ï¼šæ˜¯è¿”å›žäººç±»å¯è¯»çš„ä¿®æ”¹æ—¥æœŸ
        è¢«è°ƒç”¨ï¼šç”¨äºŽselect_deviceå‡½æ•°ä¸­ã€‚
    """
    """
        :params path: æ–‡ä»¶å é»˜è®¤å½“å‰æ–‡ä»¶
    """
    # return human-readable file modification date, i.e. '2021-3-26'
    t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)
    return f'{t.year}-{t.month}-{t.day}'

def select_device(device='', batch_size=0, newline=True):
    """
        å‡½æ•°åŠŸèƒ½ï¼šä¸»è§’ï¼Œç”¨äºŽè‡ªåŠ¨é€‰æ‹©æœ¬æœºæ¨¡åž‹è®­ç»ƒçš„è®¾å¤‡ï¼Œå¹¶è¾“å‡ºæ—¥å¿—ä¿¡æ¯ã€‚
        è¢«è°ƒç”¨ï¼štrain.pyã€test.pyã€detect.pyç­‰æ–‡ä»¶ä¸­
    """
    """
        :params device: è¾“å…¥çš„è®¾å¤‡  device = 'cpu' or '0' or '0,1,2,3'
        :params batch_size: ä¸€ä¸ªæ‰¹æ¬¡çš„å›¾ç‰‡ä¸ªæ•°
    """
    #   device = 'cpu' or '0' or '0,1,2,3'
    #   git_describe(): è¿”å›žå½“å‰æ–‡ä»¶çˆ¶æ–‡ä»¶çš„æè¿°ä¿¡æ¯(yolov5)   date_modified(): è¿”å›žå½“å‰æ–‡ä»¶çš„ä¿®æ”¹æ—¥æœŸ
    # s: ä¹‹åŽè¦åŠ å…¥loggeræ—¥å¿—çš„æ˜¾ç¤ºä¿¡æ¯
    s = f'YOLOv5 ðŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # string
    device = str(device).strip().lower().replace('cuda:', '')  # to string, 'cuda:0' to '0'

    # å¦‚æžœdeviceè¾“å…¥ä¸ºcpu  cpu=True  device.lower(): å°†deviceå­—ç¬¦ä¸²å…¨éƒ¨è½¬ä¸ºå°å†™å­—æ¯
    cpu = device == 'cpu'
    if cpu:
        # å¦‚æžœcpu=True å°±å¼ºåˆ¶(force)ä½¿ç”¨cpu ä»¤torch.cuda.is_available() = False
        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False
    elif device:  # non-cpu device requested
        # å¦‚æžœè¾“å…¥deviceä¸ä¸ºç©º  device=GPU  ç›´æŽ¥è®¾ç½® CUDA environment variable = device åŠ å…¥CUDAå¯ç”¨è®¾å¤‡
        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable - must be before assert is_available()
        # æ£€æŸ¥cudaçš„å¯ç”¨æ€§ å¦‚æžœä¸å¯ç”¨åˆ™ç»ˆæ­¢ç¨‹åº
        assert torch.cuda.is_available() and torch.cuda.device_count() >= len(device.replace(',', '')), \
            f"Invalid CUDA '--device {device}' requested, use '--device cpu' or pass valid CUDA device(s)"

    # è¾“å…¥deviceä¸ºç©º è‡ªè¡Œæ ¹æ®è®¡ç®—æœºæƒ…å†µé€‰æ‹©ç›¸åº”è®¾å¤‡  å…ˆçœ‹GPU æ²¡æœ‰å°±CPU
    # å¦‚æžœcudaå¯ç”¨ ä¸” è¾“å…¥device != cpu åˆ™ cuda=True åæ­£cuda=False
    cuda = not cpu and torch.cuda.is_available()
    if cuda:
        # devices: å¦‚æžœcudaå¯ç”¨ è¿”å›žæ‰€æœ‰å¯ç”¨çš„gpuè®¾å¤‡ i.e. 0,1,6,7  å¦‚æžœä¸å¯ç”¨å°±è¿”å›ž '0'
        devices = device.split(',') if device else '0'  # range(torch.cuda.device_count())  # i.e. 0,1,6,7
        # n: æ‰€æœ‰å¯ç”¨çš„gpuè®¾å¤‡æ•°é‡  device count
        n = len(devices)  # device count
        # æ£€æŸ¥æ˜¯å¦æœ‰gpuè®¾å¤‡ ä¸” batch_sizeæ˜¯å¦å¯ä»¥èƒ½è¢«æ˜¾å¡æ•°ç›®æ•´é™¤  check batch_size is divisible by device_count
        if n > 1 and batch_size > 0:  # check batch_size is divisible by device_count
            # å¦‚æžœä¸èƒ½åˆ™å…³é—­ç¨‹åº
            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'

        space = ' ' * (len(s) + 1) # å®šä¹‰ç­‰é•¿çš„ç©ºæ ¼

        # æ»¡è¶³æ‰€æœ‰æ¡ä»¶ såŠ ä¸Šæ‰€æœ‰æ˜¾å¡çš„ä¿¡æ¯
        for i, d in enumerate(devices):
            # p: æ¯ä¸ªå¯ç”¨æ˜¾å¡çš„ç›¸å…³å±žæ€§
            p = torch.cuda.get_device_properties(i)
            # æ˜¾ç¤ºä¿¡æ¯såŠ ä¸Šæ¯å¼ æ˜¾å¡çš„å±žæ€§ä¿¡æ¯
            s += f"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2:.0f}MiB)\n"  # bytes to MB
    else:
        # cudaä¸å¯ç”¨æ˜¾ç¤ºä¿¡æ¯så°±åŠ ä¸ŠCPU
        s += 'CPU\n'

    if not newline:
        s = s.rstrip()

    # å°†æ˜¾ç¤ºä¿¡æ¯såŠ å…¥loggeræ—¥å¿—æ–‡ä»¶ä¸­
    LOGGER.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe
    #  å¦‚æžœcudaå¯ç”¨å°±è¿”å›žç¬¬ä¸€å¼ æ˜¾å¡çš„çš„åç§° å¦‚: GeForce RTX 2060 åä¹‹è¿”å›žCPUå¯¹åº”çš„åç§°
    return torch.device('cuda:0' if cuda else 'cpu')

# ==========================================================================================================================



def device_count():
    # Returns number of CUDA devices available. Safe version of torch.cuda.device_count(). Only works on Linux.
    assert platform.system() == 'Linux', 'device_count() function only works on Linux'
    try:
        cmd = 'nvidia-smi -L | wc -l'
        return int(subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1])
    except Exception:
        return 0





def time_sync():
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨äºŽåœ¨è¿›è¡Œåˆ†å¸ƒå¼æ“ä½œæ—¶ï¼Œç²¾ç¡®è®¡ç®—å½“å‰æ—¶é—´  å¹¶è¿”å›žã€‚
        æºç åœ°å€ï¼šhttps://blog.csdn.net/qq_23981335/article/details/105709273
        è¢«è°ƒç”¨ï¼šç”¨äºŽæ•´ä¸ªé¡¹ç›®çš„ä¸­ï¼Œåªè¦æ¶‰åŠèŽ·å–å½“å‰æ—¶é—´çš„æ“ä½œï¼Œå°±éœ€è¦è°ƒç”¨è¿™ä¸ªå‡½æ•°ï¼Œå¦‚æŽ¨ç†çš„æ—¶å€™è®¡ç®—æŽ¨ç†+NMSæ‰€èŠ±è´¹æ—¶é—´ = t2 - t1ï¼š
    """
    """
        pytorch-accurate time
        å…ˆè¿›è¡Œtorch.cuda.synchronize()æ·»åŠ åŒæ­¥æ“ä½œ å†è¿”å›žtime.time()å½“å‰æ—¶é—´
        ä¸ºä»€ä¹ˆä¸ç›´æŽ¥ä½¿ç”¨time.time()å–æ—¶é—´ï¼Œè€Œè¦å…ˆæ‰§è¡ŒåŒæ­¥æ“ä½œï¼Œå†å–æ—¶é—´ï¼Ÿè¯´ä¸€ä¸‹è¿™æ ·å­åšçš„åŽŸå› :
           åœ¨pytorché‡Œé¢ï¼Œç¨‹åºçš„æ‰§è¡Œéƒ½æ˜¯å¼‚æ­¥çš„ã€‚å¦‚æžœtime.time(), æµ‹è¯•çš„æ—¶é—´ä¼šå¾ˆçŸ­ï¼Œå› ä¸ºæ‰§è¡Œå®Œend=time.time()ç¨‹åºå°±é€€å‡ºäº†
           è€Œå…ˆåŠ torch.cuda.synchronize()ä¼šå…ˆåŒæ­¥cudaçš„æ“ä½œï¼Œç­‰å¾…gpuä¸Šçš„æ“ä½œéƒ½å®Œæˆäº†å†ç»§ç»­è¿è¡Œend = time.time()
           è¿™æ ·å­æµ‹è¯•æ—¶é—´ä¼šå‡†ç¡®ã€‚
    """
    # pytorch-accurate time
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()

# ===========================================è¿™ä¸¤ä¸ªå‡½æ•°ä¸»è¦ç”¨äºŽè¾“å‡ºæ¨¡åž‹çš„ä¸€äº›ä¿¡æ¯ï¼Œå¦‚æ‰€æœ‰å±‚æ•°é‡, æ¨¡åž‹æ€»å‚æ•°é‡ç­‰========================================

def profile(input, ops, n=10, device=None):

    """
        å‡½æ•°åŠŸèƒ½ï¼šè¾“å‡ºæŸä¸ªç½‘ç»œç»“æž„ï¼ˆæ“ä½œoptsï¼‰çš„ä¸€äº›ä¿¡æ¯ï¼šæ€»å‚æ•° æµ®ç‚¹è®¡ç®—é‡ å‰å‘ä¼ æ’­æ—¶é—´ åå‘ä¼ æ’­æ—¶é—´ è¾“å…¥å˜é‡çš„shape è¾“å‡ºå˜é‡çš„shapeã€‚
    """
    """
       è¿™ä¸ªå‡½æ•°è²Œä¼¼æ²¡ç”¨åˆ°ï¼Œåœ¨ä¸‹é¢è®¡ç®—model_infoæ—¶ä¹Ÿæ˜¯è°ƒç”¨thopä¸­çš„profileå‡½æ•°ç›´æŽ¥æ‰§è¡Œçš„ï¼Œå¹¶æ²¡ç”¨ç”¨è¿™é‡Œå†™çš„è¿™ä¸ªå‡½æ•°ï¼Œæ‰€ä»¥è¿™ä¸ªå‡½æ•°å¦‚æžœä¸æƒ³çœ‹çš„è¯å…³ç³»æ˜¯ä¸å¤§çš„ã€‚
       :params x: è¾“å…¥tensor x
       :params ops: æ“ä½œops(æŸä¸ªç½‘ç»œç»“æž„)
       :params n: æ‰§è¡Œå¤šå°‘è½®ops
       :params device: æ‰§è¡Œè®¾å¤‡
       """
    # YOLOv5 speed/memory/FLOPs profiler
    #
    # Usage:
    #     input = torch.randn(16, 3, 640, 640)
    #     m1 = lambda x: x * torch.sigmoid(x)
    #     m2 = nn.SiLU()
    #     profile(input, [m1, m2], n=100)  # profile over 100 iterations

    results = []
    # é€‰æ‹©è®¾å¤‡
    device = device or select_device()
    print(f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
          f"{'input':>24s}{'output':>24s}")

    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)
        x.requires_grad = True
        for m in ops if isinstance(ops, list) else [ops]:
            m = m.to(device) if hasattr(m, 'to') else m  # device
            m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            # åˆå§‹åŒ–å‰å‘ä¼ æ’­æ—¶é—´tf åå‘ä¼ æ’­æ—¶é—´tb ä»¥åŠtå˜é‡ç”¨äºŽè®°å½•ä¸‰ä¸ªæ—¶åˆ»çš„æ—¶é—´(åŽé¢æœ‰å†™)
            tf, tb, t = 0, 0, [0, 0, 0]  # dt forward, backward
            try:
                # è®¡ç®—åœ¨è¾“å…¥ä¸ºtensor x, æ“ä½œä¸ºmæ¡ä»¶ä¸‹çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
                flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPs
            except Exception:
                flops = 0

            try:
                for _ in range(n):   # æ‰§è¡Œ100æ¬¡ ç®—å¹³å‡ æ›´å‡†ç¡®
                    t[0] = time_sync()  # æ“ä½œmå‰å‘ä¼ æ’­å‰ä¸€æ—¶åˆ»çš„æ—¶é—´
                    y = m(x)     # æ“ä½œmå‰å‘ä¼ æ’­
                    t[1] = time_sync()  # æ“ä½œmå‰å‘ä¼ æ’­åŽä¸€æ—¶åˆ»çš„æ—¶é—´ = æ“ä½œmåå‘ä¼ æ’­å‰ä¸€æ—¶åˆ»çš„æ—¶é—´
                    try:
                        # æ“ä½œmåå‘ä¼ æ’­
                        _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                        # æ“ä½œmåå‘ä¼ æ’­åŽä¸€æ—¶åˆ»çš„æ—¶é—´
                        t[2] = time_sync()
                    except Exception:  # å¦‚æžœæ²¡æœ‰åå‘ä¼ æ’­
                        # print(e)  # for debug
                        t[2] = float('nan')
                    tf += (t[1] - t[0]) * 1000 / n  # æ“ä½œmå¹³å‡æ¯æ¬¡å‰å‘ä¼ æ’­æ‰€ç”¨æ—¶é—´
                    tb += (t[2] - t[1]) * 1000 / n  # æ“ä½œmå¹³å‡æ¯æ¬¡åå‘ä¼ æ’­æ‰€ç”¨æ—¶é—´

                mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)

                # s_in: è¾“å…¥å˜é‡çš„shape
                s_in = tuple(x.shape) if isinstance(x, torch.Tensor) else 'list'
                # s_out: è¾“å‡ºå˜é‡çš„shape
                s_out = tuple(y.shape) if isinstance(y, torch.Tensor) else 'list'
                # p: mæ“ä½œ(æŸä¸ªç½‘ç»œç»“æž„)çš„æ€»å‚æ•°  parameters
                p = sum(list(x.numel() for x in m.parameters())) if isinstance(m, nn.Module) else 0  # parameters

                # è¾“å‡ºæ¯ä¸ªæ“ä½œ(æŸä¸ªç½‘ç»œç»“æž„)çš„ä¿¡æ¯: æ€»å‚æ•° æµ®ç‚¹è®¡ç®—é‡ å‰å‘ä¼ æ’­æ—¶é—´ åå‘ä¼ æ’­æ—¶é—´ è¾“å…¥å˜é‡çš„shape è¾“å‡ºå˜é‡çš„shape
                print(f'{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}')
                results.append([p, flops, mem, tf, tb, s_in, s_out])
            except Exception as e:
                print(e)
                results.append(None)
            torch.cuda.empty_cache()
    return results

def model_info(model, verbose=False, img_size=640):
    """
        å‡½æ•°åŠŸèƒ½ï¼šè¾“å‡ºæ¨¡åž‹çš„æ‰€æœ‰ä¿¡æ¯çš„ï¼ŒåŒ…æ‹¬ï¼šæ‰€æœ‰å±‚æ•°é‡, æ¨¡åž‹æ€»å‚æ•°é‡, éœ€è¦æ±‚æ¢¯åº¦çš„æ€»å‚æ•°é‡, img_sizeå¤§å°çš„modelçš„æµ®ç‚¹è®¡ç®—é‡GFLOPsã€‚
        è¢«è°ƒç”¨ï¼šyolo.pyæ–‡ä»¶çš„Modelç±»çš„infoå‡½æ•°è°ƒç”¨
    """
    """
        :params model: æ¨¡åž‹
        :params verbose: æ˜¯å¦è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
        :params img_size: int or list  i.e. img_size=640 or img_size=[640, 320]
    """
    # Model information. img_size may be int or list, i.e. img_size=640 or img_size=[640, 320]
    # n_p: æ¨¡åž‹modelçš„æ€»å‚æ•°  number parameters
    n_p = sum(x.numel() for x in model.parameters())  # number parameters
    # n_g: æ¨¡åž‹modelçš„å‚æ•°ä¸­éœ€è¦æ±‚æ¢¯åº¦(requires_grad=True)çš„å‚æ•°é‡  number gradients
    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients

    if verbose:
        # è¡¨å¤´: 'layer', 'name',  'gradient',    'parameters',    'shape',        'mu',         'sigma'
        #       ç¬¬å‡ å±‚    å±‚å   boolæ˜¯å¦éœ€è¦æ±‚æ¢¯åº¦   å½“å‰å±‚å‚æ•°é‡   å½“å‰å±‚å‚æ•°shape  å½“å‰å±‚å‚æ•°å‡å€¼    å½“å‰å±‚å‚æ•°æ–¹å·®
        print(f"{'layer':>5} {'name':>40} {'gradient':>9} {'parameters':>12} {'shape':>20} {'mu':>10} {'sigma':>10}")
        # æŒ‰è¡¨å¤´è¾“å‡ºæ¯ä¸€å±‚çš„å‚æ•°parametersçš„ç›¸å…³ä¿¡æ¯
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace('module_list.', '')
            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %
                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))

    try:  # FLOPs
        from thop import profile    # å¯¼å…¥è®¡ç®—æµ®ç‚¹è®¡ç®—é‡FLOPsçš„å·¥å…·åŒ…
        # stride æ¨¡åž‹çš„æœ€å¤§ä¸‹é‡‡æ ·çŽ‡ æœ‰[8, 16, 32] æ‰€ä»¥stride=32
        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32
        # æ¨¡æ‹Ÿä¸€æ ·è¾“å…¥å›¾ç‰‡ shape=(1, 3, 32, 32)  å…¨æ˜¯0
        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # input
        # è°ƒç”¨profileè®¡ç®—è¾“å…¥å›¾ç‰‡img=(1, 3, 32, 32)æ—¶å½“å‰æ¨¡åž‹çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs   stride GFLOPs
        # profileæ±‚å‡ºæ¥çš„æµ®ç‚¹è®¡ç®—é‡æ˜¯FLOPs  /1E9 => GFLOPs
        # *2æ˜¯å› ä¸ºprofileå‡½æ•°é»˜è®¤æ±‚çš„å°±æ˜¯æ¨¡åž‹ä¸ºfloat64æ—¶çš„æµ®ç‚¹è®¡ç®—é‡ è€Œæˆ‘ä»¬ä¼ å…¥çš„æ¨¡åž‹ä¸€èˆ¬éƒ½æ˜¯float32 æ‰€ä»¥ä¹˜ä»¥2(å¯ä»¥ç‚¹è¿›profileçœ‹ä»–å®šä¹‰çš„add_hookså‡½æ•°)
        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # stride GFLOPs
        # expand  img_size -> [img_size, img_size]=[640, 640]
        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # expand if int/float
        # æ ¹æ®img=(1, 3, 32, 32)çš„æµ®ç‚¹è®¡ç®—é‡flopsæŽ¨ç®—å‡º640x640çš„å›¾ç‰‡çš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
        # ä¸ç›´æŽ¥è®¡ç®—640x640çš„å›¾ç‰‡çš„æµ®ç‚¹è®¡ç®—é‡GFLOPså¯èƒ½æ˜¯ä¸ºäº†é«˜æ•ˆæ€§å§, è¿™æ ·ç®—å¯èƒ½é€Ÿåº¦æ›´å¿«
        fs = ', %.1f GFLOPs' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 GFLOPs
    except (ImportError, Exception):
        fs = ''
    # æ·»åŠ æ—¥å¿—ä¿¡æ¯
    # Model Summary: æ‰€æœ‰å±‚æ•°é‡, æ¨¡åž‹æ€»å‚æ•°é‡, éœ€è¦æ±‚æ¢¯åº¦çš„æ€»å‚æ•°é‡, img_sizeå¤§å°çš„modelçš„æµ®ç‚¹è®¡ç®—é‡GFLOPs
    LOGGER.info(f"Model Summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}")
# =========================================================================================================================================


def is_parallel(model):
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨äºŽåˆ¤æ–­æ¨¡åž‹æ˜¯å¦æ”¯æŒå¹¶è¡Œï¼Œ
        è¢«è°ƒç”¨ï¼šåœ¨ModelEMAç±»ä¸­
    """
    # Returns True if model is of type DP or DDP
    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)

def copy_attr(a, b, include=(), exclude=()):
    """
        å‡½æ•°åŠŸèƒ½ï¼šå¯ä»¥å°†bå¯¹è±¡çš„å±žæ€§å€¼èµ‹å€¼ç»™aå¯¹è±¡ï¼ˆkeyé”®å¿…é¡»ç›¸åŒï¼Œç„¶åŽæ‰èƒ½èµ‹å€¼ï¼‰ï¼Œå¸¸ç”¨äºŽæ¨¡åž‹èµ‹å€¼ï¼Œå¦‚ model -> emaï¼ˆModelEMAç±»å°±æ˜¯è¿™ä¹ˆå¹²çš„ï¼‰ã€‚
        è¢«è°ƒç”¨ï¼šä¸€ä¸ªæ˜¯ModelEMAç±»ä¸­ï¼Œå¦ä¸€ä¸ªæ˜¯yolo.pyæ–‡ä»¶ä¸­çš„Modelç±»çš„autoshapeå‡½æ•°

    """
    """
        :params a: å¯¹è±¡a(å¾…èµ‹å€¼)
        :params b: å¯¹è±¡b(èµ‹å€¼)
        :params include: å¯ä»¥èµ‹å€¼çš„å±žæ€§
        :params exclude: ä¸èƒ½èµ‹å€¼çš„å±žæ€§
    """
    # Copy attributes from b to a, options to only include [...] and to exclude [...]
    # __dict__è¿”å›žä¸€ä¸ªç±»çš„å®žä¾‹çš„å±žæ€§å’Œå¯¹åº”å–å€¼çš„å­—å…¸
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith('_') or k in exclude:
            continue
        else:
            # å°†å¯¹è±¡bçš„å±žæ€§kèµ‹å€¼ç»™a
            setattr(a, k, v)

class ModelEMA:
    """
        åŠŸèƒ½ï¼šéžå¸¸å¸¸è§çš„æé«˜æ¨¡åž‹é²æ£’æ€§çš„å¢žå¼ºtrockï¼Œã€‚å…¨åï¼šModel Exponential Moving Average æ¨¡åž‹çš„æŒ‡æ•°åŠ æƒå¹³å‡æ–¹æ³•ï¼Œæ˜¯ä¸€ç§ç»™äºˆè¿‘æœŸæ•°æ®æ›´é«˜æƒé‡çš„å¹³å‡æ–¹æ³•ï¼Œ
                    åˆ©ç”¨æ»‘åŠ¨å¹³å‡çš„å‚æ•°æ¥æé«˜æ¨¡åž‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„å¥å£®æ€§/é²æ£’æ€§ ï¼Œä¸€èˆ¬ç”¨äºŽæµ‹è¯•é›†ã€‚
        æºç è®²è§£ï¼š https://www.bilibili.com/video/BV1FT4y1E74V?p=63
                  https://www.cnblogs.com/wuliytTaotao/p/9479958.html
                  https://zhuanlan.zhihu.com/p/68748778
                  https://zhuanlan.zhihu.com/p/32335746
                  https://github.com/ultralytics/yolov5/issues/608
                  https://github.com/rwightman/pytorch-image-models/blob/master/timm/utils/model_ema.py            
        è¢«è°ƒç”¨ï¼šåœ¨train.pyä¸­çš„test.runï¼ˆæµ‹è¯•ï¼‰é˜¶æ®µ
    """

    def __init__(self, model, decay=0.9999, updates=0):
        """
            train.py
                model:
                decay: è¡°å‡å‡½æ•°å‚æ•°
                       é»˜è®¤0.9999 è€ƒè™‘è¿‡åŽ»10000æ¬¡çš„çœŸå®žå€¼
                updates: emaæ›´æ–°æ¬¡æ•°
        """
        # åˆ›å»ºemaæ¨¡åž‹  Create EMA
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        # if next(model.parameters()).device.type != 'cpu':
        #     self.ema.half()  # FP16 EMA
        self.updates = updates  # emaæ›´æ–°æ¬¡æ•° number of EMA updates
        # self.decay: è¡°å‡å‡½æ•° è¾“å…¥å˜é‡ä¸ºx  decay exponential ramp (to help early epochs)
        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)
        # æ‰€æœ‰å‚æ•°å–æ¶ˆè®¾ç½®æ¢¯åº¦(æµ‹è¯•  model.val)
        for p in self.ema.parameters():
            p.requires_grad_(False)

    def update(self, model):
        # æ›´æ–°emaçš„å‚æ•°  Update EMA parameters
        with torch.no_grad():
            self.updates += 1 # emaæ›´æ–°æ¬¡æ•° + 1
            d = self.decay(self.updates)    # éšç€æ›´æ–°æ¬¡æ•° æ›´æ–°å‚æ•°è´å¡”(d)

            # msd: æ¨¡åž‹é…ç½®çš„å­—å…¸ model state_dict  msdä¸­çš„æ•°æ®ä¿æŒä¸å˜ ç”¨äºŽè®­ç»ƒ
            msd = de_parallel(model).state_dict()  # model state_dict
            # éåŽ†æ¨¡åž‹é…ç½®å­—å…¸ å¦‚: k=linear.bias  v=[0.32, 0.25]  emaä¸­çš„æ•°æ®å‘ç”Ÿæ”¹å˜ ç”¨äºŽæµ‹è¯•
            for k, v in self.ema.state_dict().items():
                # è¿™é‡Œå¾—åˆ°çš„v: é¢„æµ‹å€¼
                if v.dtype.is_floating_point:
                    v *= d  # å…¬å¼å·¦è¾¹  decay * shadow_variable
                    # .detach() ä½¿å¯¹åº”çš„Variablesä¸Žç½‘ç»œéš”å¼€è€Œä¸å‚ä¸Žæ¢¯åº¦æ›´æ–°
                    v += (1 - d) * msd[k].detach()  # å…¬å¼å³è¾¹  (1âˆ’decay) * variable

    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):
        # è°ƒç”¨ä¸Šé¢çš„copy_attrå‡½æ•° ä»Žmodelä¸­å¤åˆ¶ç›¸å…³å±žæ€§å€¼åˆ°self.emaä¸­
        # Update EMA attributes
        copy_attr(self.ema, model, include, exclude)


def de_parallel(model):
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨äºŽåˆ¤æ–­å•å¡è¿˜æ˜¯å¤šå¡(èƒ½å¦å¹¶è¡Œ)å¤šå¡è¿”å›žmodel.moduleï¼Œå•å¡è¿”å›žmodelï¼ˆå…·ä½“åŽŸå› çœ‹ä¸‹é¢ä»£ç æ³¨é‡Šï¼‰ã€‚
        è¢«è°ƒç”¨ï¼šè¿™ä¸ªå‡½æ•°ç”¨åœ¨train.pyä¸­, ç”¨äºŽåŠ è½½å’Œä¿å­˜æ¨¡åž‹(å‚æ•°)ã€‚
    """
    # ç”¨åœ¨tainä¸­ä¿å­˜æ¨¡åž‹ å› ä¸ºå¤šå¡è®­ç»ƒçš„æ—¶å€™ç›´æŽ¥ç”¨model.state_dict()è¿›è¡Œä¿å­˜çš„æ¨¡åž‹, æ¯ä¸ªå±‚å‚æ•°çš„åç§°å‰é¢ä¼šåŠ ä¸Šmodule,
    # è¿™æ—¶å€™å†ç”¨å•å¡(gpu) model_dictåŠ è½½model.state_dict()å‚æ•°æ—¶ä¼šå‡ºçŽ°åç§°ä¸åŒ¹é…çš„æƒ…å†µ,
    # å› æ­¤å¤šå¡ä¿å­˜æ¨¡åž‹æ—¶æ³¨æ„ä½¿ç”¨model.module.state_dict() å³è¿”å›žmodel.module  å•å¡è¿”å›žmodelå³å¯
    return model.module if is_parallel(model) else model


def initialize_weights(model):
    """
        å‡½æ•°åŠŸèƒ½ï¼šåˆå§‹åŒ–æ¨¡åž‹æƒé‡çš„ï¼Œ
        è¢«è°ƒç”¨ï¼šyolo.pyçš„Modelç±»ä¸­çš„initå‡½æ•°
    """
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:  # å¦‚æžœæ˜¯äºŒç»´å·ç§¯å°±è·³è¿‡  æˆ–è€…  ä½¿ç”¨ä½•å‡¯æ˜Žåˆå§‹åŒ–
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d: # å¦‚æžœæ˜¯BNå±‚ å°±è®¾ç½®ç›¸å…³å‚æ•°: epså’Œmomentum
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:
            # å¦‚æžœæ˜¯è¿™å‡ ç±»æ¿€æ´»å‡½æ•° inplaceæ’å€¼å°±èµ‹ä¸ºTrue
            # inplace = True æŒ‡è¿›è¡ŒåŽŸåœ°æ“ä½œ å¯¹äºŽä¸Šå±‚ç½‘ç»œä¼ é€’ä¸‹æ¥çš„tensorç›´æŽ¥è¿›è¡Œä¿®æ”¹ ä¸éœ€è¦å¦å¤–èµ‹å€¼å˜é‡
            # è¿™æ ·å¯ä»¥èŠ‚çœè¿ç®—å†…å­˜ï¼Œä¸ç”¨å¤šå‚¨å­˜å˜é‡
            m.inplace = True


def find_modules(model, mclass=nn.Conv2d):
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨äºŽæ‰¾åˆ°æ¨¡åž‹modelä¸­ç±»åž‹æ˜¯mclassçš„å±‚ç»“æž„çš„ç´¢å¼•
    """
    """
       è¿™ä¸ªå‡½æ•°æ²¡ç”¨åˆ°ï¼Œæ‰€ä»¥å¤§å¯ä¸çœ‹ã€‚
       :params model: æ¨¡åž‹
       :params mclass: å±‚ç»“æž„ç±»åž‹ é»˜è®¤nn.Conv2d
   """
    # Finds layer indices matching module class 'mclass'
    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]


# =====================================================è¿™ä¸¤ä¸ªå‡½æ•°å®žçŽ°æ¨¡åž‹å‰ªæžï¼Œä½œè€…å¹¶æ²¡æœ‰ä½¿ç”¨ï¼Œä½†è‡ªå·±æ˜¯å¯åœ¨test.pyå’Œdetect.pyä¸­è¿›è¡Œæ¨¡åž‹å‰ªæžå®žéªŒ===============================
def sparsity(model):
    """
        å‡½æ•°åŠŸèƒ½ï¼šç”¨æ¥è®¡ç®—æ¨¡åž‹çš„ç¨€ç–ç¨‹åº¦sparsityï¼Œè¿”å›žæ¨¡åž‹æ•´ä½“çš„ç¨€ç–æ€§ã€‚
        è¢«è°ƒç”¨ï¼špruneå‰ªæžå‡½æ•°ä¸­
    """
    # åˆå§‹åŒ–æ¨¡åž‹çš„æ€»å‚æ•°ä¸ªæ•°a(å‰å‘+åå‘)  æ¨¡åž‹å‚æ•°ä¸­å€¼ä¸º0çš„å‚æ•°ä¸ªæ•°b
    # Return global model sparsity
    a, b = 0, 0
    # model.parameters()è¿”å›žæ¨¡åž‹modelçš„å‚æ•° è¿”å›žä¸€ä¸ªç”Ÿæˆå™¨ éœ€è¦ç”¨forå¾ªçŽ¯æˆ–è€…next()æ¥èŽ·å–å‚æ•°
    # forå¾ªçŽ¯å–å‡ºæ¯ä¸€å±‚çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å‚æ•°
    for p in model.parameters():
        a += p.numel()
        b += (p == 0).sum()
    # b / a å³å¯ä»¥ååº”æ¨¡åž‹çš„ç¨€ç–ç¨‹åº¦
    return b / a

def prune(model, amount=0.3):
    """
        å‡½æ•°åŠŸèƒ½ï¼šè¿™ä¸ªå‡½æ•°æ˜¯ç”¨äºŽå¯¹æ¨¡åž‹modelè¿›è¡Œå‰ªæžçš„ï¼Œé€šè¿‡è°ƒç”¨sparsityå‡½æ•°è®¡ç®—æ¨¡åž‹çš„ç¨€ç–æ€§è¿›è¡Œå‰ªæžï¼Œä»¥å¢žåŠ æ¨¡åž‹çš„ç¨€ç–æ€§ã€‚
        å…·ä½“ç”¨æ³•ï¼šhttps://github.com/ultralytics/yolov5/issues/304
        è¢«è°ƒç”¨ï¼šç”¨äºŽtest.pyå’Œdetect.pyä¸­è¿›è¡Œæ¨¡åž‹å‰ªæž
        å…³äºŽå‡½æ•°çš„ç”¨æ³•ä½œè€…å¹¶æ²¡æœ‰å†™åœ¨å®ƒçš„ä»£ç ä¸­ï¼Œä¸è¿‡å¹¶ä¸å¦¨ç¢æˆ‘ä»¬è‡ªå·±å®žéªŒå®ƒï¼Œå…·ä½“ç”¨æ³•å¯ä»¥æŸ¥çœ‹ä¸‹é¢æ ‡æ³¨çš„é“¾æŽ¥ã€‚å®ƒä¸»è¦æ˜¯å¯ä»¥ç”¨åœ¨ä¸¤ä¸ªåœ°æ–¹ï¼š
    """

    """
        :params model: æ¨¡åž‹
        :params amount: éšæœºè£å‰ª(æ€»å‚æ•°é‡ x amount)æ•°é‡çš„å‚æ•°
    """
    # Prune model to requested global sparsity
    import torch.nn.utils.prune as prune    # å¯¼å…¥ç”¨äºŽå‰ªæžçš„å·¥å…·åŒ…
    print('Pruning model... ', end='')

    # æ¨¡åž‹çš„è¿­ä»£å™¨ è¿”å›žçš„æ˜¯æ‰€æœ‰æ¨¡å—çš„è¿­ä»£å™¨  åŒæ—¶äº§ç”Ÿæ¨¡å—çš„åç§°(name)ä»¥åŠæ¨¡å—æœ¬èº«(m)
    for name, m in model.named_modules():
        if isinstance(m, nn.Conv2d):
            # å¯¹å½“å‰å±‚ç»“æž„m, éšæœºè£å‰ª(æ€»å‚æ•°é‡ x amount)æ•°é‡çš„æƒé‡(weight)å‚æ•°
            prune.l1_unstructured(m, name='weight', amount=amount)  # prune
            # å½»åº•ç§»é™¤è¢«è£å‰ªçš„çš„æƒé‡å‚æ•°
            prune.remove(m, 'weight')  # make permanent

    #  è¾“å‡ºæ¨¡åž‹çš„ç¨€ç–åº¦ è°ƒç”¨sparsityå‡½æ•°è®¡ç®—å½“å‰æ¨¡åž‹çš„ç¨€ç–åº¦
    print(' %.3g global sparsity' % sparsity(model))
# ============================================================================================================================================


def fuse_conv_and_bn(conv, bn):
    """
        å‡½æ•°åŠŸèƒ½ï¼šæ˜¯ä¸€ä¸ªå¢žå¼ºæ–¹å¼ï¼Œæ€æƒ³å°±æ˜¯ï¼šåœ¨æŽ¨ç†æµ‹è¯•é˜¶æ®µï¼Œå°†å·ç§¯å±‚å’ŒBNå±‚è¿›è¡Œèžåˆï¼Œä»¥åŠ é€ŸæŽ¨ç†ã€‚
        å‡½æ•°åŽŸç†ï¼š æ–¹æ³•: å·ç§¯å±‚è¿˜æ˜¯æ­£å¸¸å®šä¹‰, ä½†æ˜¯å·ç§¯å±‚çš„å‚æ•°w,bè¦æ”¹å˜   é€šè¿‡åªæ”¹å˜å·ç§¯å‚æ•°, è¾¾åˆ°CONV+BNçš„æ•ˆæžœ
                w = w_bn * w_conv   b = w_bn * b_conv + b_bn   (å¯ä»¥è¯æ˜Ž)
        æºç åœ°å€ï¼šhttps://tehnokv.com/posts/fusing-batchnorm-and-conv/
                 https://github.com/ultralytics/yolov3/issues/807
                 https://zhuanlan.zhihu.com/p/94138640
        è¢«è°ƒç”¨ï¼šåœ¨yolo.pyä¸­Modelç±»çš„fuseå‡½æ•°ä¸­è°ƒç”¨
    """
    """
        :params conv: torchæ”¯æŒçš„å·ç§¯å±‚
        :params bn: torchæ”¯æŒçš„bnå±‚
    """
    # Fuse convolution and batchnorm layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/
    fusedconv = nn.Conv2d(conv.in_channels,
                          conv.out_channels,
                          kernel_size=conv.kernel_size,
                          stride=conv.stride,
                          padding=conv.padding,
                          groups=conv.groups,
                          bias=True).requires_grad_(False).to(conv.weight.device)

    # prepare filters
    # w_conv: å·ç§¯å±‚çš„wå‚æ•° ç›´æŽ¥clone convçš„weightå³å¯
    w_conv = conv.weight.clone().view(conv.out_channels, -1)
    # w_bn: bnå±‚çš„wå‚æ•°(å¯ä»¥è‡ªå·±æŽ¨åˆ°å…¬å¼)  torch.diag: è¿”å›žä¸€ä¸ªä»¥inputä¸ºå¯¹è§’çº¿å…ƒç´ çš„2D/1D æ–¹é˜µ/å¼ é‡?
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    # w = w_bn * w_conv      torch.mm: å¯¹ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # prepare spatial bias
    # b_conv: å·ç§¯å±‚çš„bå‚æ•° å¦‚æžœä¸ä¸ºNoneå°±ç›´æŽ¥è¯»å–conv.biaså³å¯
    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias
    # b_bn: bnå±‚çš„bå‚æ•°(å¯ä»¥è‡ªå·±æŽ¨åˆ°å…¬å¼)
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    #  b = w_bn * b_conv + b_bn   (w_bn not forgot)
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv

def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)
    """
        å‡½æ•°åŠŸèƒ½ï¼šè¿™ä¸ªå‡½æ•°æ˜¯ç”¨äºŽå¯¹å›¾ç‰‡è¿›è¡Œç¼©æ”¾æ“ä½œï¼ŒTest Time Augmentation(TTA)æ“ä½œå°±æ˜¯åœ¨æµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨æ•°æ®å¢žå¼ºï¼Œä¹Ÿç®—æ˜¯ä¸€ç§å¢žå¼ºçš„æ–¹å¼å§ã€‚
        è¢«è°ƒç”¨ï¼šç”¨äºŽyolo.pyæ–‡ä»¶ä¸­Modelç±»çš„forward_augmentå‡½æ•°ä¸­
    """

    """
        :params img: åŽŸå›¾
        :params ratio: ç¼©æ”¾æ¯”ä¾‹ é»˜è®¤=1.0 åŽŸå›¾
        :params same_shape: ç¼©æ”¾ä¹‹åŽå°ºå¯¸æ˜¯å¦æ˜¯è¦æ±‚çš„å¤§å°(å¿…é¡»æ˜¯gs=32çš„å€æ•°)
        :params gs: æœ€å¤§çš„ä¸‹é‡‡æ ·çŽ‡ 32 æ‰€ä»¥ç¼©æ”¾åŽçš„å›¾ç‰‡çš„shapeå¿…é¡»æ˜¯gs=32çš„å€æ•°
    """
    # img(16,3,256,416)
    # scales img(bs,3,y,x) by ratio constrained to gs-multiple
    if ratio == 1.0:    # å¦‚æžœç¼©æ”¾æ¯”ä¾‹ratioä¸º1.0 ç›´æŽ¥è¿”å›žåŽŸå›¾
        return img
    else:   # å¦‚æžœç¼©æ”¾æ¯”ä¾‹ratioä¸ä¸º1.0 åˆ™å¼€å§‹æ ¹æ®ç¼©æ”¾æ¯”ä¾‹ratioè¿›è¡Œç¼©æ”¾
        # h, w: åŽŸå›¾çš„é«˜å’Œå®½
        h, w = img.shape[2:]
        # s: æ”¾ç¼©åŽå›¾ç‰‡çš„æ–°å°ºå¯¸  new size
        s = (int(h * ratio), int(w * ratio))  # new size
        # ç›´æŽ¥ä½¿ç”¨torchè‡ªå¸¦çš„F.interpolate(ä¸Šé‡‡æ ·ä¸‹é‡‡æ ·å‡½æ•°)æ’å€¼å‡½æ•°è¿›è¡Œresize
        # F.interpolate: å¯ä»¥ç»™å®šsizeæˆ–è€…scale_factoræ¥è¿›è¡Œä¸Šä¸‹é‡‡æ ·
        #                mode='bilinear': åŒçº¿æ€§æ’å€¼  nearest:æœ€è¿‘é‚»
        #                align_corner: æ˜¯å¦å¯¹é½ input å’Œ output çš„è§’ç‚¹åƒç´ (corner pixels)
        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize
        if not same_shape:  # pad/crop img
            # ç¼©æ”¾ä¹‹åŽè¦æ˜¯å°ºå¯¸å’Œè¦æ±‚çš„å¤§å°(å¿…é¡»æ˜¯gs=32çš„å€æ•°)ä¸åŒ å†å¯¹å…¶ä¸ç›¸äº¤çš„éƒ¨åˆ†è¿›è¡Œpad
            # è€Œpadçš„å€¼å°±æ˜¯imagenetçš„mean
            # Math.ceil(): å‘ä¸Šå–æ•´  è¿™é‡Œé™¤ä»¥gså‘ä¸Šå–æ•´å†ä¹˜ä»¥gsæ˜¯ä¸ºäº†ä¿è¯hã€wéƒ½æ˜¯gsçš„å€æ•°
            h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
        # pad img shape to gsçš„å€æ•° å¡«å……å€¼ä¸º imagenet mean
        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean

class EarlyStopping:
    # YOLOv5 simple early stopper
    def __init__(self, patience=30):
        self.best_fitness = 0.0  # i.e. mAP
        self.best_epoch = 0
        self.patience = patience or float('inf')  # epochs to wait after fitness stops improving to stop
        self.possible_stop = False  # possible stop may occur next epoch

    def __call__(self, epoch, fitness):
        if fitness >= self.best_fitness:  # >= 0 to allow for early zero-fitness stage of training
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # epochs without improvement
        self.possible_stop = delta >= (self.patience - 1)  # possible stop may occur next epoch
        stop = delta >= self.patience  # stop training if patience exceeded
        if stop:
            LOGGER.info(f'Stopping training early as no improvement observed in last {self.patience} epochs. '
                        f'Best results observed at epoch {self.best_epoch}, best model saved as best.pt.\n'
                        f'To update EarlyStopping(patience={self.patience}) pass a new patience value, '
                        f'i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.')
        return stop